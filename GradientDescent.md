# Gradient Descent

Gradient descent is a first order iterative optimization algorithm used for finding the minimum of a function. 
First order in that it uses the first differential of a function, iterative as in it requires multiple steps to work.
We look for a minimum of a function by finding the first order differential which tells us the gradient at a point.
Starting from a random point, we then take steps along the negative gradient with the assumtion that after many 
small steps along the negative gradient we will eventually end up to the minimum of the function.
